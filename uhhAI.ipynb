{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716c9e7-d1f7-4ee4-be1e-6654add08ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # `pip3 install assemblyai` (macOS)\n",
    "# # `pip install assemblyai` (Windows)\n",
    "\n",
    "# import assemblyai as aai\n",
    "\n",
    "# aai.settings.api_key = \"804b4c0e83a940f5aaf099c7486000ad\"\n",
    "# transcriber = aai.Transcriber()\n",
    "\n",
    "# transcript = transcriber.transcribe(\"um.mp4\")\n",
    "# # transcript = transcriber.transcribe(\"./my-local-audio-file.wav\")\n",
    "\n",
    "# print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca9ccc-7ff7-457d-80a8-d56d8bc7e5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42505179-3c02-47a6-aad3-c6665d53da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ibm_watson import SpeechToTextV1\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# authenticator = IAMAuthenticator(\"YOUR_API_KEY\")\n",
    "# speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "\n",
    "# audio_file = \"path/to/audio/file.wav\"\n",
    "\n",
    "# with open(audio_file, \"rb\") as audio_file:\n",
    "#     result = speech_to_text.recognize(audio=audio_file, content_type=\"audio/wav\")\n",
    "#     print(\"Transcript: {}\".format(result.result[\"results\"][0][\"alternatives\"][0][\"transcript\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b629a9-6399-4ab0-bb23-34dc934cdb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80e79582-5a92-4ab6-aa43-976b10537aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" How's it going guys? This is Joe. I'm back again to do an audio test. And what we're looking to do is make a new editing tool that will allow me to process my edits, remove lengthy spaces. and then also remove words like um maybe I'm wasting my time on this but I guess I just wanted to know how I need to know how to deploy models and just just kind of working my way up the ladder, I guess.\"}\n",
      "Transcriptions:  How's it going guys? This is Joe. I'm back again to do an audio test. And what we're looking to do is make a new editing tool that will allow me to process my edits, remove lengthy spaces. and then also remove words like um maybe I'm wasting my time on this but I guess I just wanted to know how I need to know how to deploy models and just just kind of working my way up the ladder, I guess.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydub import AudioSegment\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from pydub.silence import split_on_silence\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import librosa\n",
    "import requests\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Step 1: Extract audio from video using pydub\n",
    "video_path = \"test2.mp4\"\n",
    "output_audio_path = \"audio.wav\"\n",
    "output_text_path = \"transcript.txt\"\n",
    "output_video_path = \"processed_video.mp4\"\n",
    "\n",
    "# Load video with pydub\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "\n",
    "# Export audio to desired format (16kHz mono)\n",
    "audio.export(output_audio_path, format=\"wav\")\n",
    "\n",
    "\n",
    "\n",
    "# API_TOKEN = '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'\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai/whisper-small\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(result)\n",
    "        # Access the transcriptions from the result\n",
    "        transcriptions = result.get(\"text\", [])\n",
    "        \n",
    "        # Print or process the transcriptions as needed\n",
    "        print(\"Transcriptions:\", transcriptions)\n",
    "        \n",
    "        return transcriptions\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "# output = query(\"sample1.flac\")\n",
    "\n",
    "output = query(\"audio.wav\")\n",
    "\n",
    "# Example usage:\n",
    "# output = query_audio(\"audio.wav\")\n",
    "# print(output)\n",
    "\n",
    "# # Assuming 'output' contains the JSON object\n",
    "# response_json = output\n",
    "\n",
    "# # Extract the transcription\n",
    "# transcription = response_json\n",
    "# # For simplicity, you can print the entire response to inspect its structure\n",
    "\n",
    "\n",
    "# # Now you can use the transcription for further processing or analysis\n",
    "# print(\"Transcription:\", transcription)\n",
    "\n",
    "\n",
    "# # Extract detailed timing information (replace with actual structure)\n",
    "# timing_info = response_json.get('words', [])\n",
    "\n",
    "# # Set a pause threshold (in seconds)\n",
    "# pause_threshold = 2.0\n",
    "\n",
    "# # Split the transcription based on pauses and align with timing information\n",
    "# segments = []\n",
    "# current_segment = \"\"\n",
    "# current_start_time = 0\n",
    "\n",
    "# # Iterate through timing information\n",
    "# for word_info in timing_info:\n",
    "#     if word_info.get(\"case\") == \"success\":\n",
    "#         current_word = word_info['alternatives'][0]['word']\n",
    "#         current_end_time = word_info['alternatives'][0]['end']\n",
    "\n",
    "#         # Check if a pause is detected\n",
    "#         if current_start_time > 0 and (current_end_time - current_start_time) > pause_threshold:\n",
    "#             # Process the current segment\n",
    "#             segments.append(current_segment.strip())\n",
    "#             current_segment = \"\"\n",
    "        \n",
    "#         current_segment += current_word + \" \"\n",
    "#         current_start_time = current_end_time\n",
    "\n",
    "# # Process the last segment\n",
    "# segments.append(current_segment.strip())\n",
    "\n",
    "# # Now 'segments' contains the transcription segments aligned with timing information\n",
    "# print(\"Aligned Segments:\", segments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88d40785-abc1-418d-9035-0f123c919eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arrays to stack must be passed as a \"sequence\" type such as list or tuple.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m target_sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[0;32m     17\u001b[0m resampler \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResample(audio\u001b[38;5;241m.\u001b[39mfps, target_sampling_rate)\n\u001b[1;32m---> 18\u001b[0m resampled_audio \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mapply_codec(resampler, \u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_soundarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Step 3: Convert resampled audio to text using the Whisper model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(resampled_audio, sampling_rate\u001b[38;5;241m=\u001b[39mtarget_sampling_rate)\n",
      "File \u001b[1;32m<decorator-gen-62>:2\u001b[0m, in \u001b[0;36mto_soundarray\u001b[1;34m(self, tt, fps, quantize, nbytes, buffersize)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\moviepy\\decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\moviepy\\audio\\AudioClip.py:113\u001b[0m, in \u001b[0;36mAudioClip.to_soundarray\u001b[1;34m(self, tt, fps, quantize, nbytes, buffersize)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m>\u001b[39m max_duration:\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstacker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         tt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mfps)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\numpy\\core\\shape_base.py:216\u001b[0m, in \u001b[0;36m_vhstack_dispatcher\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_vhstack_dispatcher\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_arrays_for_stack_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\numpy\\core\\shape_base.py:209\u001b[0m, in \u001b[0;36m_arrays_for_stack_dispatcher\u001b[1;34m(arrays)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arrays_for_stack_dispatcher\u001b[39m(arrays):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arrays, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrays to stack must be passed as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    210\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuch as list or tuple.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(arrays)\n",
      "\u001b[1;31mTypeError\u001b[0m: arrays to stack must be passed as a \"sequence\" type such as list or tuple."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from moviepy.editor import VideoFileClip\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Step 1: Load pre-trained processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"whisper-small\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"whisper-small\")\n",
    "\n",
    "# Step 2: Load video and extract audio\n",
    "video_path = \"test2.mp4\"\n",
    "video = VideoFileClip(video_path)\n",
    "audio = video.audio\n",
    "\n",
    "# Resample audio to the model's expected sampling rate (16000 Hz)\n",
    "target_sampling_rate = 16000\n",
    "resampler = T.Resample(audio.fps, target_sampling_rate)\n",
    "resampled_audio = torchaudio.functional.apply_codec(resampler, audio.to_soundarray().T.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Step 3: Convert resampled audio to text using the Whisper model\n",
    "inputs = processor(resampled_audio, sampling_rate=target_sampling_rate)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Step 4: Get the transcription\n",
    "transcription = processor.batch_decode(outputs.logits, skip_special_tokens=True)\n",
    "\n",
    "# Step 5: Print or use the transcription as needed\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32f497a-ebc6-4820-8da4-de45c4b767f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Waveform: tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0007, -0.0007, -0.0007]])\n",
      "Sample Rate: 44100\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "from transformers import AutoProcessor\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "from pydub.playback import play\n",
    "from pydub.silence import split_on_silence\n",
    "import torchaudio\n",
    "import torch\n",
    "import requests\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"whisper-small\")\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"whisper-small\")\n",
    "\n",
    "# Step 2: Load video and extract audio\n",
    "video_path = \"test2.mp4\"\n",
    "video = VideoFileClip(video_path)\n",
    "\n",
    "audio_path = \"audio.wav\"\n",
    "video.audio.write_audiofile(audio_path)\n",
    "\n",
    "# Load the audio using torchaudio\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "\n",
    "# Print the loaded waveform and sample rate\n",
    "print(\"Waveform:\", waveform)\n",
    "print(\"Sample Rate:\", sample_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c100704-e501-4487-9986-2488dc0f8aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled Waveform: tensor([[ 2.3691e-07, -3.0055e-07,  2.5742e-05,  ..., -1.0690e-03,\n",
      "         -1.0793e-03, -8.4070e-04],\n",
      "        [ 2.3691e-07, -3.0055e-07,  2.5742e-05,  ..., -1.0690e-03,\n",
      "         -1.0793e-03, -8.4070e-04]])\n",
      "Resampled Sample Rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# Preprocess audio (resample, convert to mono, normalize, etc.)\n",
    "# Example: Resample to 16 kHz\n",
    "resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform_resampled = resample(waveform)\n",
    "\n",
    "# Print the resampled waveform and its sample rate\n",
    "print(\"Resampled Waveform:\", waveform_resampled)\n",
    "print(\"Resampled Sample Rate:\", 16000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effa260e-c041-4c87-a554-4cb7fa972fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 6.25000846e-05 1.25000169e-04 ... 4.61498750e+01\n",
      " 4.61499375e+01 4.61500000e+01]\n"
     ]
    }
   ],
   "source": [
    "# Calculate timestamps based on the length of the audio\n",
    "duration = waveform_resampled.size(1) / 16000  # Duration in seconds\n",
    "timestamps = np.linspace(0, duration, waveform_resampled.size(1))\n",
    "\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "105abccb-b907-4a94-810e-89563e8caa30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000e+00, 6.2500e-05, 1.2500e-04,  ..., 4.6150e+01,\n",
      "          4.6150e+01, 4.6150e+01]],\n",
      "\n",
      "        [[0.0000e+00, 6.2500e-05, 1.2500e-04,  ..., 4.6150e+01,\n",
      "          4.6150e+01, 4.6150e+01]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Add timestamps as an additional channel to the features\n",
    "timestamps_tensor = torch.tensor([timestamps]).expand(waveform_resampled.size(0), -1, -1)\n",
    "print(timestamps_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e8541-822d-415b-be63-d67fc565bfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d235c18-7b62-4ac2-add7-590e42fd82a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_features': tensor([[[-0.2279, -0.0604,  0.1427,  ...,  0.1518,  0.3445,  0.4917],\n",
      "         [-0.4554, -0.2644,  0.0890,  ...,  0.3403,  0.2929,  0.5090],\n",
      "         [-0.3842, -0.4717, -0.0074,  ...,  0.3680,  0.3273,  0.6232],\n",
      "         ...,\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.4701, -0.6017, -0.3724],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5170, -0.5487, -0.4044],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5889, -0.6212, -0.4283]],\n",
      "\n",
      "        [[-0.2279, -0.0604,  0.1427,  ...,  0.1518,  0.3445,  0.4917],\n",
      "         [-0.4554, -0.2644,  0.0890,  ...,  0.3403,  0.2929,  0.5090],\n",
      "         [-0.3842, -0.4717, -0.0074,  ...,  0.3680,  0.3273,  0.6232],\n",
      "         ...,\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.4701, -0.6017, -0.3724],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5170, -0.5487, -0.4044],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5889, -0.6212, -0.4283]]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(waveform_resampled\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[1;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 4: Get the transcription\u001b[39;00m\n\u001b[0;32m      7\u001b[0m transcription \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39mlogits, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1818\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[1;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1815\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1816\u001b[0m         )\n\u001b[1;32m-> 1818\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1829\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1836\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1694\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[1;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1687\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1688\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1689\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1690\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1691\u001b[0m     )\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1694\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newdev\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1436\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1434\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m past_key_values_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "# Tokenize and obtain transcriptions\n",
    "inputs = processor(waveform_resampled.squeeze().numpy(), return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "\n",
    "print(inputs)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Step 4: Get the transcription\n",
    "transcription = processor.batch_decode(outputs.logits, skip_special_tokens=True)\n",
    "\n",
    "# Step 5: Print or use the transcription as needed\n",
    "print(transcription)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a342fc36-578b-4e4c-bcbe-5b38f9ba06d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 738400 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Add timestamps as an additional channel to the features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_features_with_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform_resampled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 738400 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Add timestamps as an additional channel to the features\n",
    "input_features_with_timestamps = torch.cat((waveform_resampled.unsqueeze(-1), timestamps_tensor), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f831971b-4fb3-46bf-bb3f-14bb8df7fca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform shape: (2, 738400)\n",
      "Processed Inputs: {'input_features': tensor([[[-0.2279, -0.0604,  0.1427,  ...,  0.1518,  0.3445,  0.4917],\n",
      "         [-0.4554, -0.2644,  0.0890,  ...,  0.3403,  0.2929,  0.5090],\n",
      "         [-0.3842, -0.4717, -0.0074,  ...,  0.3680,  0.3273,  0.6232],\n",
      "         ...,\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.4701, -0.6017, -0.3724],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5170, -0.5487, -0.4044],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5889, -0.6212, -0.4283]],\n",
      "\n",
      "        [[-0.2279, -0.0604,  0.1427,  ...,  0.1518,  0.3445,  0.4917],\n",
      "         [-0.4554, -0.2644,  0.0890,  ...,  0.3403,  0.2929,  0.5090],\n",
      "         [-0.3842, -0.4717, -0.0074,  ...,  0.3680,  0.3273,  0.6232],\n",
      "         ...,\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.4701, -0.6017, -0.3724],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5170, -0.5487, -0.4044],\n",
      "         [-0.7912, -0.7912, -0.7912,  ..., -0.5889, -0.6212, -0.4283]]])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the KMP_DUPLICATE_LIB_OK environment variable\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "\n",
    "# Tokenize the audio\n",
    "inputs = processor(waveform_resampled.squeeze().numpy(), return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "# Print intermediate information\n",
    "print(\"Waveform shape:\", waveform_resampled.squeeze().numpy().shape)\n",
    "print(\"Processed Inputs:\", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791777a4-0654-4e28-8ec6-750d35efe30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
