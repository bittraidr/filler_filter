{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad125b16-6e89-40de-98e8-671bd4a9c9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import moviepy.editor as mp\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.silence import split_on_silence\n",
    "# from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "# import numpy as np\n",
    "\n",
    "# # Set log level to suppress Vosk log messages\n",
    "# SetLogLevel(0)\n",
    "\n",
    "# # Step 1: Extract audio from video\n",
    "# video = mp.VideoFileClip(\"um.mp4\")\n",
    "# audio = video.audio\n",
    "\n",
    "# # Step 2: Convert audio to WAV for speech recognition\n",
    "# audio.export(\"audio.wav\", format=\"wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# # Step 3: Speech recognition using Vosk\n",
    "# model_path = \"../vosk-model-en-us-0.22\"\n",
    "# try:\n",
    "#     model = Model(model_path)\n",
    "#     print(\"Model loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     # Handle the error or exit the program\n",
    "\n",
    "# # Step 4: Process audio chunks\n",
    "# audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# chunks = split_on_silence(audio_data, silence_thresh=-40)\n",
    "# recognized_text = \"\"\n",
    "\n",
    "# for chunk in chunks:\n",
    "#     # Convert audio data to numpy array of 16-bit signed integers\n",
    "#     audio_np = np.array(chunk.get_array_of_samples(), dtype=np.int16)\n",
    "#     recognizer = KaldiRecognizer(model, audio_np)\n",
    "#     recognized_text += recognizer.FinalResult()\n",
    "\n",
    "# # Step 5: Identify \"um\" occurrences and cut corresponding video segments\n",
    "# um_segments = [(start, end) for start, end in zip(chunks[:-1], chunks[1:]) if \"um\" in recognized_text[start:end]]\n",
    "\n",
    "# edited_segments = []\n",
    "# for start, end in um_segments:\n",
    "#     edited_segments.append(video.subclip(start / 1000, end / 1000))  # Convert milliseconds to seconds\n",
    "\n",
    "# final_video = mp.concatenate_videoclips(edited_segments)\n",
    "\n",
    "# # Step 6: Export final video\n",
    "# final_video.write_videofile(\"umRemove.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d867d93a-7af5-4088-adbb-f6eb5bea7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m chunk_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Process audio chunks\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mchunks\u001b[49m):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Print the duration of each chunk\u001b[39;00m\n\u001b[0;32m     59\u001b[0m      \u001b[38;5;66;03m# Export the individual chunk to a separate audio file\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mexport(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;241m.\u001b[39mduration_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import wit\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move one directory up from the current directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Construct the path to witToken.txt\n",
    "wit_path = os.path.join(parent_dir, 'witToken.csv')\n",
    "\n",
    "# Read the access token from witToken.txt\n",
    "with open(wit_path, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row:\n",
    "            access_token = row[0]\n",
    "\n",
    "# Set up Wit.ai\n",
    "wit.access_token = access_token\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "video = mp.VideoFileClip(\"um.mp4\")\n",
    "audio = video.audio\n",
    "\n",
    "# Step 2: Convert audio to WAV for speech recognition\n",
    "audio.write_audiofile(\"audio.wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# Step 3: Process audio chunks\n",
    "audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# Adjust parameters for split_on_silence\n",
    "# Adjust parameters for split_on_silence\n",
    "# chunks = split_on_silence(\n",
    "#     audio_data,\n",
    "#     silence_thresh=-40,\n",
    "#     min_silence_len=500,    # Reduce the minimum silence length\n",
    "#     silence_chunk_len=500,  # Length of each chunk (if a chunk has no speech, it will be extended up to this length)\n",
    "#     keep_silence=100,       # Keep this amount of silence at the beginning and end of each chunk\n",
    "# )\n",
    "\n",
    "recognized_text = \"\"\n",
    "\n",
    "# Directory to save individual audio chunks\n",
    "output_directory = \"audio_chunks\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store start and end times of each chunk\n",
    "chunk_times = []\n",
    "\n",
    "# Process audio chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Print the duration of each chunk\n",
    "     # Export the individual chunk to a separate audio file\n",
    "    chunk.export(os.path.join(output_directory, f\"chunk_{i + 1}.wav\"), format=\"wav\")\n",
    "    print(f\"Chunk {i + 1} duration: {chunk.duration_seconds} seconds\")\n",
    "\n",
    "    # Convert audio data to numpy array of 16-bit signed integers\n",
    "    audio_np = chunk.get_array_of_samples()\n",
    "\n",
    "    # Upload the audio to Wit.ai for recognition\n",
    "    wit_url = \"https://api.wit.ai/speech\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"audio/wav\",\n",
    "    }\n",
    "    response = requests.post(wit_url, headers=headers, data=chunk.raw_data)\n",
    "\n",
    "    # Print the response from Wit.ai API\n",
    "    print(f\"Response from Wit.ai API: {response.json()}\")\n",
    "\n",
    "    # Extract recognized text from the response\n",
    "    recognized_text += response.json().get(\"_text\", \"\")\n",
    "\n",
    "    # Store the start and end times of the chunk\n",
    "    start_time = sum(chunk_times[-1]) if chunk_times else 0  # Calculate cumulative duration\n",
    "    end_time = start_time + chunk.duration_seconds\n",
    "    chunk_times.append((start_time, end_time))\n",
    "\n",
    "# Save the complete transcript to a text file\n",
    "with open(\"transcript.txt\", \"w\") as transcript_file:\n",
    "    transcript_file.write(recognized_text)\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716c9e7-d1f7-4ee4-be1e-6654add08ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # `pip3 install assemblyai` (macOS)\n",
    "# # `pip install assemblyai` (Windows)\n",
    "\n",
    "# import assemblyai as aai\n",
    "\n",
    "# aai.settings.api_key = \"804b4c0e83a940f5aaf099c7486000ad\"\n",
    "# transcriber = aai.Transcriber()\n",
    "\n",
    "# transcript = transcriber.transcribe(\"um.mp4\")\n",
    "# # transcript = transcriber.transcribe(\"./my-local-audio-file.wav\")\n",
    "\n",
    "# print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca9ccc-7ff7-457d-80a8-d56d8bc7e5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import wit\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move one directory up from the current directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Construct the path to witToken.txt\n",
    "wit_path = os.path.join(parent_dir, 'witToken.csv')\n",
    "\n",
    "# Read the access token from witToken.txt\n",
    "with open(wit_path, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row:\n",
    "            access_token = row[0]\n",
    "\n",
    "# Set up Wit.ai\n",
    "wit.access_token = access_token\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "video = mp.VideoFileClip(\"um.mp4\")\n",
    "audio = video.audio\n",
    "\n",
    "# Step 2: Convert audio to WAV for speech recognition\n",
    "audio.write_audiofile(\"audio.wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# Step 3: Process audio chunks\n",
    "audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# Replace split_on_silence with manual processing\n",
    "chunks = audio_data.split_to_mono()\n",
    "\n",
    "recognized_text = \"\"\n",
    "\n",
    "# Directory to save individual audio chunks\n",
    "output_directory = \"audio_chunks\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store start and end times of each chunk\n",
    "chunk_times = []\n",
    "\n",
    "# Word to be removed\n",
    "word_to_remove = b\"ummm\"\n",
    "\n",
    "# Process audio chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Export the individual chunk to a separate audio file\n",
    "    chunk.export(os.path.join(output_directory, f\"chunk_{i + 1}.wav\"), format=\"wav\")\n",
    "    print(f\"Chunk {i + 1} duration: {chunk.duration_seconds} seconds\")\n",
    "\n",
    "    # Convert audio data to numpy array of 16-bit signed integers\n",
    "    audio_np = chunk.get_array_of_samples()\n",
    "\n",
    "    # Use local speech recognition\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_chunk = sr.AudioData(audio_np, chunk.frame_rate, 2)\n",
    "    try:\n",
    "        recognized_text_chunk = recognizer.recognize_google(audio_chunk)\n",
    "        print(f\"Recognized text from local recognition: {recognized_text_chunk}\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio\")\n",
    "        recognized_text_chunk = \"\"\n",
    "\n",
    "    # Remove the specified word from the recognized text\n",
    "    recognized_text_chunk = recognized_text_chunk.replace(word_to_remove.decode(), \"\")\n",
    "\n",
    "    # Append the modified chunk text to the complete recognized text\n",
    "    recognized_text += recognized_text_chunk\n",
    "    \n",
    "    # Save the recognized text for each chunk to a separate text file\n",
    "    with open(os.path.join(output_directory, f\"transcript_chunk_{i + 1}.txt\"), \"w\") as transcript_file:\n",
    "        transcript_file.write(recognized_text_chunk)\n",
    "\n",
    "    # Store the start and end times of the chunk\n",
    "    start_time = sum(chunk_times[-1]) if chunk_times else 0  # Calculate cumulative duration\n",
    "    end_time = start_time + chunk.duration_seconds\n",
    "    chunk_times.append((start_time, end_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42505179-3c02-47a6-aad3-c6665d53da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ibm_watson import SpeechToTextV1\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# authenticator = IAMAuthenticator(\"YOUR_API_KEY\")\n",
    "# speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "\n",
    "# audio_file = \"path/to/audio/file.wav\"\n",
    "\n",
    "# with open(audio_file, \"rb\") as audio_file:\n",
    "#     result = speech_to_text.recognize(audio=audio_file, content_type=\"audio/wav\")\n",
    "#     print(\"Transcript: {}\".format(result.result[\"results\"][0][\"alternatives\"][0][\"transcript\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e79582-5a92-4ab6-aa43-976b10537aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from pydub import AudioSegment\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "processor = AutoProcessor.from_pretrained(\"Baghdad99/saad-speech-recognition-hausa-audio-to-text\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"Baghdad99/saad-speech-recognition-hausa-audio-to-text\")\n",
    "\n",
    "# Step 1: Extract audio from video using pydub\n",
    "video_path = \"um.mp4\"\n",
    "output_audio_path = \"audio.wav\"\n",
    "output_text_path = \"transcript.txt\"\n",
    "output_video_path = \"processed_video.mp4\"\n",
    "\n",
    "# Load video with pydub\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "\n",
    "# Export audio to desired format (16kHz mono)\n",
    "audio.export(output_audio_path, format=\"wav\")\n",
    "\n",
    "\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(output_audio_path)\n",
    "\n",
    "# Step 3: Preprocess the audio file for the model\n",
    "input_values = processor(waveform.numpy(), return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "# Step 4: Perform inference using the loaded model\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Step 5: Decode the logits to text\n",
    "transcription = processor.batch_decode(logits, skip_special_tokens=False)[0]\n",
    "\n",
    "# Step 6: Save the transcription to a text file\n",
    "with open(output_text_path, \"w\") as f:\n",
    "    f.write(transcription)\n",
    "\n",
    "# Optionally, you can also save the processed audio if needed\n",
    "processed_waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "torchaudio.save(\"processed_audio.wav\", processed_waveform, sample_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd28499-1624-40ff-b6a7-d22d579a2c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
