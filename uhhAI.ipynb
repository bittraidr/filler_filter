{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad125b16-6e89-40de-98e8-671bd4a9c9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import moviepy.editor as mp\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.silence import split_on_silence\n",
    "# from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "# import numpy as np\n",
    "\n",
    "# # Set log level to suppress Vosk log messages\n",
    "# SetLogLevel(0)\n",
    "\n",
    "# # Step 1: Extract audio from video\n",
    "# video = mp.VideoFileClip(\"um.mp4\")\n",
    "# audio = video.audio\n",
    "\n",
    "# # Step 2: Convert audio to WAV for speech recognition\n",
    "# audio.export(\"audio.wav\", format=\"wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# # Step 3: Speech recognition using Vosk\n",
    "# model_path = \"../vosk-model-en-us-0.22\"\n",
    "# try:\n",
    "#     model = Model(model_path)\n",
    "#     print(\"Model loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     # Handle the error or exit the program\n",
    "\n",
    "# # Step 4: Process audio chunks\n",
    "# audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# chunks = split_on_silence(audio_data, silence_thresh=-40)\n",
    "# recognized_text = \"\"\n",
    "\n",
    "# for chunk in chunks:\n",
    "#     # Convert audio data to numpy array of 16-bit signed integers\n",
    "#     audio_np = np.array(chunk.get_array_of_samples(), dtype=np.int16)\n",
    "#     recognizer = KaldiRecognizer(model, audio_np)\n",
    "#     recognized_text += recognizer.FinalResult()\n",
    "\n",
    "# # Step 5: Identify \"um\" occurrences and cut corresponding video segments\n",
    "# um_segments = [(start, end) for start, end in zip(chunks[:-1], chunks[1:]) if \"um\" in recognized_text[start:end]]\n",
    "\n",
    "# edited_segments = []\n",
    "# for start, end in um_segments:\n",
    "#     edited_segments.append(video.subclip(start / 1000, end / 1000))  # Convert milliseconds to seconds\n",
    "\n",
    "# final_video = mp.concatenate_videoclips(edited_segments)\n",
    "\n",
    "# # Step 6: Export final video\n",
    "# final_video.write_videofile(\"umRemove.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867d93a-7af5-4088-adbb-f6eb5bea7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import moviepy.editor as mp\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.silence import split_on_silence\n",
    "# import wit\n",
    "# import requests\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "# # Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# # Move one directory up from the current directory\n",
    "# parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# # Construct the path to witToken.txt\n",
    "# wit_path = os.path.join(parent_dir, 'witToken.csv')\n",
    "\n",
    "# # Read the access token from witToken.txt\n",
    "# with open(wit_path, 'r') as file:\n",
    "#     csv_reader = csv.reader(file)\n",
    "#     for row in csv_reader:\n",
    "#         if row:\n",
    "#             access_token = row[0]\n",
    "\n",
    "# # Set up Wit.ai\n",
    "# wit.access_token = access_token\n",
    "\n",
    "# # Step 1: Extract audio from video\n",
    "# video = mp.VideoFileClip(\"um.mp4\")\n",
    "# audio = video.audio\n",
    "\n",
    "# # Step 2: Convert audio to WAV for speech recognition\n",
    "# audio.write_audiofile(\"audio.wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# # Step 3: Process audio chunks\n",
    "# audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# # Adjust parameters for split_on_silence\n",
    "# # Adjust parameters for split_on_silence\n",
    "# # chunks = split_on_silence(\n",
    "# #     audio_data,\n",
    "# #     silence_thresh=-40,\n",
    "# #     min_silence_len=500,    # Reduce the minimum silence length\n",
    "# #     silence_chunk_len=500,  # Length of each chunk (if a chunk has no speech, it will be extended up to this length)\n",
    "# #     keep_silence=100,       # Keep this amount of silence at the beginning and end of each chunk\n",
    "# # )\n",
    "\n",
    "# recognized_text = \"\"\n",
    "\n",
    "# # Directory to save individual audio chunks\n",
    "# output_directory = \"audio_chunks\"\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# # Initialize a list to store start and end times of each chunk\n",
    "# chunk_times = []\n",
    "\n",
    "# # Process audio chunks\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     # Print the duration of each chunk\n",
    "#      # Export the individual chunk to a separate audio file\n",
    "#     chunk.export(os.path.join(output_directory, f\"chunk_{i + 1}.wav\"), format=\"wav\")\n",
    "#     print(f\"Chunk {i + 1} duration: {chunk.duration_seconds} seconds\")\n",
    "\n",
    "#     # Convert audio data to numpy array of 16-bit signed integers\n",
    "#     audio_np = chunk.get_array_of_samples()\n",
    "\n",
    "#     # Upload the audio to Wit.ai for recognition\n",
    "#     wit_url = \"https://api.wit.ai/speech\"\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {access_token}\",\n",
    "#         \"Content-Type\": \"audio/wav\",\n",
    "#     }\n",
    "#     response = requests.post(wit_url, headers=headers, data=chunk.raw_data)\n",
    "\n",
    "#     # Print the response from Wit.ai API\n",
    "#     print(f\"Response from Wit.ai API: {response.json()}\")\n",
    "\n",
    "#     # Extract recognized text from the response\n",
    "#     recognized_text += response.json().get(\"_text\", \"\")\n",
    "\n",
    "#     # Store the start and end times of the chunk\n",
    "#     start_time = sum(chunk_times[-1]) if chunk_times else 0  # Calculate cumulative duration\n",
    "#     end_time = start_time + chunk.duration_seconds\n",
    "#     chunk_times.append((start_time, end_time))\n",
    "\n",
    "# # Save the complete transcript to a text file\n",
    "# with open(\"transcript.txt\", \"w\") as transcript_file:\n",
    "#     transcript_file.write(recognized_text)\n",
    "\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2716c9e7-d1f7-4ee4-be1e-6654add08ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a test. 1212. So this is the test. Testing whether I can remove the word and continue with my software. If I can make this, that'd be super cool. It'd be very useful for editing and to resplice it back together. That'd be dope. Or just use the software that's already out there. Every time I have a cool idea and I go and look online, somebody's already fucking done it.\n"
     ]
    }
   ],
   "source": [
    "# `pip3 install assemblyai` (macOS)\n",
    "# `pip install assemblyai` (Windows)\n",
    "\n",
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = \"804b4c0e83a940f5aaf099c7486000ad\"\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "transcript = transcriber.transcribe(\"um.mp4\")\n",
    "# transcript = transcriber.transcribe(\"./my-local-audio-file.wav\")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca9ccc-7ff7-457d-80a8-d56d8bc7e5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import wit\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move one directory up from the current directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Construct the path to witToken.txt\n",
    "wit_path = os.path.join(parent_dir, 'witToken.csv')\n",
    "\n",
    "# Read the access token from witToken.txt\n",
    "with open(wit_path, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row:\n",
    "            access_token = row[0]\n",
    "\n",
    "# Set up Wit.ai\n",
    "wit.access_token = access_token\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "video = mp.VideoFileClip(\"um.mp4\")\n",
    "audio = video.audio\n",
    "\n",
    "# Step 2: Convert audio to WAV for speech recognition\n",
    "audio.write_audiofile(\"audio.wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# Step 3: Process audio chunks\n",
    "audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "# Replace split_on_silence with manual processing\n",
    "chunks = audio_data.split_to_mono()\n",
    "\n",
    "recognized_text = \"\"\n",
    "\n",
    "# Directory to save individual audio chunks\n",
    "output_directory = \"audio_chunks\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store start and end times of each chunk\n",
    "chunk_times = []\n",
    "\n",
    "# Word to be removed\n",
    "word_to_remove = b\"ummm\"\n",
    "\n",
    "# Process audio chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Export the individual chunk to a separate audio file\n",
    "    chunk.export(os.path.join(output_directory, f\"chunk_{i + 1}.wav\"), format=\"wav\")\n",
    "    print(f\"Chunk {i + 1} duration: {chunk.duration_seconds} seconds\")\n",
    "\n",
    "    # Convert audio data to numpy array of 16-bit signed integers\n",
    "    audio_np = chunk.get_array_of_samples()\n",
    "\n",
    "    # Use local speech recognition\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_chunk = sr.AudioData(audio_np, chunk.frame_rate, 2)\n",
    "    try:\n",
    "        recognized_text_chunk = recognizer.recognize_google(audio_chunk)\n",
    "        print(f\"Recognized text from local recognition: {recognized_text_chunk}\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio\")\n",
    "        recognized_text_chunk = \"\"\n",
    "\n",
    "    # Remove the specified word from the recognized text\n",
    "    recognized_text_chunk = recognized_text_chunk.replace(word_to_remove.decode(), \"\")\n",
    "\n",
    "    # Append the modified chunk text to the complete recognized text\n",
    "    recognized_text += recognized_text_chunk\n",
    "    \n",
    "    # Save the recognized text for each chunk to a separate text file\n",
    "    with open(os.path.join(output_directory, f\"transcript_chunk_{i + 1}.txt\"), \"w\") as transcript_file:\n",
    "        transcript_file.write(recognized_text_chunk)\n",
    "\n",
    "    # Store the start and end times of the chunk\n",
    "    start_time = sum(chunk_times[-1]) if chunk_times else 0  # Calculate cumulative duration\n",
    "    end_time = start_time + chunk.duration_seconds\n",
    "    chunk_times.append((start_time, end_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9defe46-5383-45f4-824b-391b414a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify \"um\" occurrences and cut corresponding video segments\n",
    "um_segments = [(start, end) for (start, end), chunk in zip(chunk_times, chunks) if \"um\" in recognized_text]\n",
    "\n",
    "\n",
    "edited_segments = []\n",
    "\n",
    "for start, end in um_segments:\n",
    "    edited_segments.append(video.subclip(start / 1000, end / 1000))  # Convert milliseconds to seconds\n",
    "\n",
    "final_video = mp.concatenate_videoclips(edited_segments)\n",
    "\n",
    "# Step 5: Export final video\n",
    "final_video.write_videofile(\"umRemove.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "# Clean up Wit.ai\n",
    "wit.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42505179-3c02-47a6-aad3-c6665d53da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ibm_watson import SpeechToTextV1\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# authenticator = IAMAuthenticator(\"YOUR_API_KEY\")\n",
    "# speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "\n",
    "# audio_file = \"path/to/audio/file.wav\"\n",
    "\n",
    "# with open(audio_file, \"rb\") as audio_file:\n",
    "#     result = speech_to_text.recognize(audio=audio_file, content_type=\"audio/wav\")\n",
    "#     print(\"Transcript: {}\".format(result.result[\"results\"][0][\"alternatives\"][0][\"transcript\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e79582-5a92-4ab6-aa43-976b10537aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Name\\anaconda3\\envs\\newdev\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import wit\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load model directly\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "video = mp.VideoFileClip(\"um.mp4\")\n",
    "audio = video.audio\n",
    "\n",
    "# Step 2: Convert audio to WAV for speech recognition\n",
    "audio.write_audiofile(\"audio.wav\", codec=\"pcm_s16le\")\n",
    "\n",
    "# Step 3: Process audio chunks\n",
    "audio_data = AudioSegment.from_wav(\"audio.wav\")\n",
    "\n",
    "# Replace split_on_silence with manual processing\n",
    "# Split audio into chunks (adjust chunk_size and overlap as needed)\n",
    "chunk_size = 5000  # in milliseconds\n",
    "overlap = 500  # in milliseconds\n",
    "chunks = [audio_data[i : i + chunk_size] for i in range(0, len(audio_data), chunk_size - overlap)]\n",
    "\n",
    "# Recognize speech for each chunk\n",
    "recognized_text = \"\"\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.export(f\"audio_chunks/chunk_{i}.wav\", format=\"wav\")\n",
    "\n",
    "    # Read audio data from the file\n",
    "    audio_path = f\"audio_chunks/chunk_{i}.wav\"\n",
    "    audio_data_chunk = AudioSegment.from_wav(audio_path)\n",
    "    audio_data_chunk.export(audio_path, format=\"wav\")\n",
    "\n",
    "    # Use the Whisper ASR model to transcribe each chunk\n",
    "    inputs = processor(audio_data_chunk.get_array_of_samples(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values=inputs[\"input_values\"]).logits\n",
    "    transcriptions = processor.batch_decode(logits)\n",
    "\n",
    "    # Process and append transcriptions to the recognized_text\n",
    "    transcribed_text = transcriptions[0].lower()  # Assuming one transcription per chunk\n",
    "    # Remove specific words like 'umm' and 'so'\n",
    "    transcribed_text = transcribed_text.replace('umm', '').replace('so', '')\n",
    "    recognized_text += transcribed_text\n",
    "\n",
    "# Additional processing on recognized_text to remove specific words or patterns\n",
    "# ...\n",
    "\n",
    "# Print or save the final recognized text\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd28499-1624-40ff-b6a7-d22d579a2c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
